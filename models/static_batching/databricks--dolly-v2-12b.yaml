deployment_config:
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 90.0
  ray_actor_options:
    resources:
      accelerator_type_cpu: 0.01
model_config:
  batching: static
  model_id: databricks/dolly-v2-12b
  model_description: "Databricksâ€™ dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. Based on pythia-12b, Dolly is trained on ~15k instruction/response fine tuning records databricks-dolly-15k generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization. dolly-v2-12b is not a state-of-the-art model, but does exhibit surprisingly high quality instruction following behavior not characteristic of the foundation model on which it is based.\n\nDolly v2 is also available in these smaller models sizes:\n\ndolly-v2-7b, a 6.9 billion parameter based on pythia-6.9b\ndolly-v2-3b, a 2.8 billion parameter based on pythia-2.8b\nPlease refer to the dolly GitHub repo for tips on running inference for various GPU configurations."
  initialization:
    s3_mirror_config:
      bucket_uri: s3://large-dl-models-mirror/models--databricks--dolly-v2-12b/main-safetensors/
      s3_sync_args:
        - "--no-sign-request"
    initializer:
      type: DeepSpeed
      dtype: float16
      from_pretrained_kwargs:
        use_cache: true
      use_kernel: true
      max_tokens: 1536
    pipeline: transformers
  generation:
    max_input_words: 800
    max_batch_size: 4
    generate_kwargs:
      do_sample: true
      max_new_tokens: 512
      min_new_tokens: 16
      top_p: 0.92
      top_k: 0
      temperature: 1.0
    prompt_format:
      system: "{instruction}\n"
      assistant: "### Response:\n{instruction}\n"
      trailing_assistant: "### Response:\n"
      user: "### Instruction:\n{instruction}\n"
      default_system_message: "Below is an instruction that describes a task. Write a response that appropriately completes the request."
    stopping_sequences: ["### Response:", "### End"]
scaling_config:
  num_workers: 2
  num_gpus_per_worker: 1
  num_cpus_per_worker: 4
  resources_per_worker:
    accelerator_type_a10: 0.01
