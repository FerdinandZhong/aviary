enabled: false
deployment_config:
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 90.0
  ray_actor_options:
    resources:
      accelerator_type_cpu: 0.01
model_config:
  batching: static
  model_id: eachadea/ggml-vicuna-13b-1.1
  initialization:
    runtime_env:
      env_vars:
        FORCE_CMAKE: "1"
        CMAKE_ARGS: "-DLLAMA_BLAS=on"
        # For GPU support, do this instead:
        # FORCE_CMAKE: "1"
        # CMAKE_ARGS: "-DLLAMA_CUBLAS=on"
        # cuBLAS llama.cpp will throw exceptions on CPU only nodes.
        # Also make sure to change the scaling config below.
      pip:
        - "llama-cpp-python>=0.1.63"
    initializer:
      type: LlamaCpp
      model_filename: "ggml-vic13b-q5_1.bin"
      model_init_kwargs:
        n_ctx: 2048
    pipeline: llamacpp
  generation:
    max_batch_size: 1
    generate_kwargs:
      max_tokens: 512
      min_new_tokens: 16
      temperature: 0.2
    prompt_format:
      system: "{instruction} "
      assistant: "ASSISTANT: {instruction} "
      trailing_assistant: "ASSISTANT: "
      user: "USER: {instruction} "
      default_system_message: "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. The assistant is knowledgeable, polite and creative and doesn't write code unless asked to."
    stopping_sequences: ["USER:"]
scaling_config:
  num_workers: 1
  num_gpus_per_worker: 0
  num_cpus_per_worker: 4
  resources_per_worker:
    accelerator_type_cpu: 0.01